### 全文检索 ###
* 为什么不试用普通数据库检索方式？
* 全文检索（试想给一篇文章分词）
 * 理解成给一本书设置目录（根据与目录就知道内容在哪里）
 * 目录：索引 ,给谁建立索引？给内容（文章）设置索引。先给文章分词（给分出的词设置索引（post实体），）
 * 分词
 * 创建索引
* django提供的全文检索类库
 * django-haystack(容器)
 * whoosh（具体搜索的）
* django-haystack支持Solr,Elasticsearch,Whoosh, Xapian四种搜索引擎
 *  安装
	pip install django-haystack
	pip insatll whoosh
 * settings中配置haystack

						
				INSTALLED_APPS = [
				    'django.contrib.admin',
				    'django.contrib.auth',
				    'django.contrib.contenttypes',
				    'django.contrib.sessions',
				    'django.contrib.messages',
				    'django.contrib.staticfiles',
				    'haystack',
				    'post',
				    'ckeditor',
				    'ckeditor_uploader'
				]
 * 给haystack配置搜索引擎


			HAYSTACK_CONNECTIONS = {  
			    'default': {  
			        'ENGINE': 'haystack.backends.whoosh_backend.WhooshEngine',  
			        'PATH': os.path.join(BASE_DIR, 'whoosh_index'),  
			    },  
			}  
 * 创建索引（类比书的目录） search_indexes.py文件(给药搜索的实体设置索引)

* 已经产生了whoosh_index索引文件，
* 读取whoosh_index索引文件产生ix对象。
* q = QueryParser(keyword)
* ix.searcher()搜索器ix.searcher().search(q)
 
				#coding=UTF-8
				from  haystack import indexes
				from post.models import *
				#注意格式
				class PostIndex(indexes.SearchIndex,indexes.Indexable):
				    text = indexes.CharField(document=True, use_template=True)
				    #给title设置索引
				    title = indexes.NgramField(model_attr='title')
				    content = indexes.NgramField(model_attr='content')
				    def get_model(self):
				        return Post
				    def index_queryset(self, using=None):
				        return self.get_model().objects.order_by('-created').all()

 * 建立搜索引擎模板（templates/search/indexes/yourapp/post_text.txt）
				

			{{object.title}} 
			{{object.content}}


 * 生成索引文件
  * python manage.py rebuild_index
  * python manage.py update_index (如果查询不到内容) 

 * 检索代码



				def search_view(request):
				    from haystack.query import SQ
				    search_result =  SearchQuerySet().filter(SQ(title = request.GET.get('q')) | SQ(content = request.GET.get('a'))).all()
				    # SearchResult().object
				    posts=[]
				    for result in search_result:
				       posts.append(result.object)
				    return render(request,'archive.html',{'posts':posts})
 * 分页 pagintor




					def search_view(request):
					    from haystack.query import SQ
					    search_result =  SearchQuerySet().filter(SQ(title = request.GET.get('q'))).all()
					    # SearchResult().object
					    from  django.core.paginator import  Paginator
					    paginator = Paginator(search_result,10)
					    posts = paginator.page(1).object_list
					    print  len(posts)
					    # posts=[]
					    # for result in search_result:
					    #    posts.append(result.object)
					    return render(request,'archive.html',{'posts':posts})
 * 自动更新索引
 
			#添加此项，当数据库改变时，会自动更新索引，非常方便
			HAYSTACK_SIGNAL_PROCESSOR = 'haystack.signals.RealtimeSignalProcessor'  
 * 中文分词的引入，就是将一句话分成很多正确的词（jieba[http://blog.csdn.net/wenxuansoft/article/details/8169842](http://blog.csdn.net/wenxuansoft/article/details/8169842)）
 
			pip install jieba
 * 自定义中文分词器



				from whoosh.analysis import Tokenizer,Token
					class ChineseTokenizer(Tokenizer):
					    def __call__(self, value, positions=False, chars=False,
					                 keeporiginal=False, removestops=True,
					                 start_pos=0, start_char=0, mode='', **kwargs):
					        assert isinstance(value, text_type), "%r is not unicode" % value
					        t = Token(positions, chars, removestops=removestops, mode=mode,
					            **kwargs)
					        seglist=jieba.cut(value,cut_all=False)                       #使用结巴分词库进行分词
					        for w in seglist:
					            t.original = t.text = w
					            t.boost = 1.0
					            if positions:
					                t.pos=start_pos+value.find(w)
					            if chars:
					                t.startchar=start_char+value.find(w)
					                t.endchar=start_char+value.find(w)+len(w)
					            yield t                                               #通过生成器返回每个分词的结果token
					
					def ChineseAnalyzer():
					    return ChineseTokenizer()
					
					
					#重点在这里，将原先的RegexAnalyzer(ur”([\u4e00-\u9fa5])|(\w+(\.?\w+)*)”),改成这句，用中文分词器代替原先的正则表达式解释器。
					analyzer=ChineseAnalyzer()


 * 自定义分词器后端 [http://blog.csdn.net/wenxuansoft/article/details/8170714](http://blog.csdn.net/wenxuansoft/article/details/8170714)
 * 拷贝\Lib\site-packages\haystack\backends目录whoosh_backend到app目录下，改名whoosh_cn_backend.py 
 * 修改如下
 
			  schema_fields[field_class.index_fieldname] = TEXT(stored=True, analyzer=StemmingAnalyzer(), field_boost=field_class.boost, sortable=True)

				#改为自定义的ChineseAnalyzer
				
				 schema_fields[field_class.index_fieldname] = TEXT(stored=True, analyzer=ChineseAnalyzer(), field_boost=field_class.boost, sortable=True)

 * 修改settings中设置
				
				
				HAYSTACK_CONNECTIONS = {
							    'default': {
							        'ENGINE': 'post.whoosh_cn_backend.WhooshEngine',
							        'PATH': os.path.join(BASE_DIR, 'whoosh_index'),
							    },
				}
* 高亮（标签）


 * 重建索引(在ChinessAnalyzer中设置打印，检查是否执行分词，根据分词搜索数据)
### 虚拟环境配置 ###
* 开一个虚拟环境（针对于python项目）
* 只有一个2.7版本，所有的pip安装的组建都会到scripts中，如果一个项目需要django1.11.6 另一个需要django1.8.2出问题
* 独立的使用不同的python+django
*  pip install virtualenv 安装虚拟环境
*  mkdir myproject
*  cd myproject
*   virtualenv  venv 创建一个独立的虚拟环境，没有任何的其他包
*   source venv/bin/activate 激活环境（virtualenv会修改相关环境变量，让命令python和pip均指向当前的virtualenv环境。）
*   virtualenv是如何创建“独立”的Python运行环境的呢？原理很简单，就是把系统Python复制一份到virtualenv的环境，用命令source venv/bin/activate进入一个virtualenv环境时，virtualenv会修改相关环境变量，让命令python和pip均指向当前的virtualenv环境。
*   deactivate  退出虚拟环境

### pythonanywhere ###
* 注册账号
* 上传文件，解压zip

			unzip 文件
			rm  ***.zip
			mv ***/* /
			rm ***
			
* 添加web
* 修改设置。
* 重新部署



### 集成友言 ###
* 注册账号


		<!-- UY BEGIN -->
		<div id="uyan_frame"></div>
		<script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=2148171"></script>
		<!-- UY END -->
* 只能在线测试
 

								
				 
